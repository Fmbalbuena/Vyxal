package vyxal.catslexer

import scala.language.strictEquality

// Import everything but Token
import vyxal.{Token as _, *}
import vyxal.catslexer.TokenType.*
import vyxal.impls.Elements

import scala.collection.mutable
import scala.collection.mutable.{ListBuffer, Queue}
import scala.util.matching.Regex

import cats.parse.Parser as P

/** Lexer for literate mode. Use [[LiterateLexer.apply]] */
private[catslexer] object LiterateLexer:
  private val endKeywords = List(
    "endfor",
    "end-for",
    "endwhile",
    "end-while",
    "endlambda",
    "end-lambda",
    "end",
  )

  private val branchKeywords = List(
    ":",
    ",",
    "else",
    "elif",
    "else-if",
    "body",
    "do",
    "branch",
    "->",
    "then",
    "in",
    "using",
  )

  /** Map keywords to their token types */
  private val keywords = Map(
    "close-all" -> TokenType.StructureAllClose
  )

  private val lambdaOpeners = Map(
    "{" -> StructureType.Lambda,
    "lambda" -> StructureType.Lambda,
    "lam" -> StructureType.Lambda,
    "map-lambda" -> StructureType.LambdaMap,
    "map-lam" -> StructureType.LambdaMap,
    "filter-lambda" -> StructureType.LambdaFilter,
    "filter-lam" -> StructureType.LambdaFilter,
    "sort-lambda" -> StructureType.LambdaSort,
    "sort-lam" -> StructureType.LambdaSort,
    "reduce-lambda" -> StructureType.LambdaReduce,
    "reduce-lam" -> StructureType.LambdaReduce,
    "fold-lambda" -> StructureType.LambdaReduce,
    "fold-lam" -> StructureType.LambdaReduce
  )

  /** Keywords for opening structures. Has to be a separate map because while
    * all of them have the same [[TokenType]], they have different values
    * depending on the kind of structure
    */
  private val structOpeners = Map(
    // These can't go in the big map, because that's autogenerated
    "?" -> StructureType.Ternary,
    "?->" -> StructureType.Ternary,
    "if" -> StructureType.IfStatement,
    "for" -> StructureType.For,
    "do-to-each" -> StructureType.For,
    "while" -> StructureType.While,
    "is-there?" -> StructureType.DecisionStructure,
    "does-exist?" -> StructureType.DecisionStructure,
    "is-there" -> StructureType.DecisionStructure,
    "does-exist" -> StructureType.DecisionStructure,
    "any-in" -> StructureType.DecisionStructure,
    "relation" -> StructureType.GeneratorStructure,
    "generate-from" -> StructureType.GeneratorStructure,
    "generate" -> StructureType.GeneratorStructure,
  )

  lazy val literateModeMappings: Map[String, String] =
    Elements.elements.values.view.flatMap { elem =>
      elem.keywords.map(_ -> elem.symbol)
    }.toMap ++ Modifiers.modifiers.view.flatMap { (symbol, mod) =>
      mod.keywords.map(_ -> symbol)
    }.toMap ++ keywords.map { (kw, typ) =>
      kw -> typ.canonicalSBCS.get
    }.toMap ++ endKeywords
      .map(_ -> TokenType.StructureClose.canonicalSBCS.get)
      .toMap ++ branchKeywords
      .map(_ -> TokenType.Branch.canonicalSBCS.get)
      .toMap ++ (lambdaOpeners ++ structOpeners).map { (kw, typ) =>
      kw -> typ.open
    }

  /** Tokenize a piece of code in literate mode */
  def lex(code: String): Either[VyxalCompilationError, List[Token]] =
    tokens.parseAll(code).left.map(err => VyxalCompilationError(err.toString()))

  def isList(code: String): Boolean =
    list.parseAll(code).successful

  private def sbcsifySingle(token: Token): String =
    val Token(tokenType, value, _) = token
    tokenType match
      case GetVar => "#$" + value
      case SetVar => s"#=$value"
      case AugmentVar => s"#>$value"
      case Constant => s"#!$value"
      case Str => s""""$value""""
      case SyntaxTrigraph if value == ":=[" => "#:["
      case Command if !Elements.elements.contains(value) =>
        Elements.symbolFor(value).get
      case _ => tokenType.canonicalSBCS.getOrElse(value)

  /** Convert literate mode code into SBCS mode code */
  def sbcsify(tokens: List[Token]): String =
    val out = StringBuilder()

    for i <- tokens.indices do
      val token @ Token(tokenType, value, _) = tokens(i)
      val sbcs = sbcsifySingle(token)
      out.append(sbcs)

      if i < tokens.length - 1 then
        val next = tokens(i + 1)
        tokenType match
          case Number =>
            if value != "0" && next.tokenType == Number
              && next.value != "." && !value.endsWith(".")
            then out.append(" ")
          case GetVar | SetVar | AugmentVar | Constant =>
            if "[a-zA-Z0-9_]+".r.matches(sbcsifySingle(next)) then
              out.append(" ")
          case _ =>
    end for

    out.toString
  end sbcsify

  def ws: P[List[Token]] = "[ \t\r\f]+|##[^\n]*".r ^^^ Nil

  private val litDecimalRegex =
    raw"(-?((0|[1-9][0-9_]*)?\.[0-9]*|0|[1-9][0-9_]*))"
  override def number: P[Token] =
    withRange(
      raw"(${litDecimalRegex}i($litDecimalRegex)?)|(i$litDecimalRegex)|$litDecimalRegex|(i( |$$))".r
    ) ^^ { case (value, range) =>
      val temp = value.replace("i", "ı").replace("_", "")
      val parts =
        if !temp.endsWith("ı") then temp.split("ı").toSeq
        else temp.init.split("ı").toSeq :+ ""
      Token(
        Number,
        parts
          .map(x => if x.startsWith("-") then x.substring(1) + "_" else x)
          .mkString("ı"),
        range
      )
    }

  override def string: P[Token] =
    withRange(raw"""("(?:[^"\\]|\\.)*")""".r) ^^ { case (value, range) =>
      Token(Str, value.substring(1, value.length - 1), range)
    }

  override def contextIndex: P[Token] =
    withRange("""`\d*`""".r) ^^ { case (value, range) =>
      Token(ContextIndex, value.substring(1, value.length - 2), range)
    }

  def lambdaBlock: P[List[Token]] =
    keywordsParser(
      StructureType.lambdaStructures.map(_.open) ++ lambdaOpeners.keys
    )
      ~! ( // Keep going until the branch indicating params end, but don't stop at ","
        rep(not((branch | litBranch).filter(_.value != ",")) ~> singleToken)
          .map(_.flatten)
          ~ (branch | litBranch)
      ).?
      ~ rep(
        not(
          litStructClose | structureSingleClose | structureDoubleClose | structureAllClose
        ) ~> singleToken
      ).map(_.flatten)
      ~ (litStructClose | structureSingleClose | structureDoubleClose | not(
        not(structureAllClose)
      )).? ^^ { case (opener, openRange) ~ possibleParams ~ body ~ endTok =>
        val openerTok =
          Token(
            StructureOpen,
            // If it's a keyword, map it to SBCS
            lambdaOpeners.get(opener).map(_.open).getOrElse(opener),
            openRange
          )
        val possParams = possibleParams match
          case Some(params ~ branch) =>
            // Branches get turned into `|` when sbcsifying. To preserve commas, turn them into Commands instead
            val paramsWithCommas = params.map(tok =>
              if tok.tokenType == Branch && tok.value == "," then
                Token(Command, ",", tok.range)
              else if tok.tokenType == Command then tok.copy(tokenType = Param)
              else tok
            )
            paramsWithCommas :+ branch
          case None => Nil
        val withoutEnd = openerTok :: (possParams ::: body)
        endTok match
          case Some(tok: Token) => withoutEnd :+ tok
          case _ =>
            // This means there was a StructureAllClose or we hit EOF
            withoutEnd
      }
  end lambdaBlock

  def litListOpen: P[Token] = withRange("[") ^^ { case (_, range) =>
    Token(ListOpen, "#[", range)
  }

  def litListClose: P[Token] = withRange("]") ^^ { case (_, range) =>
    Token(ListClose, "#]", range)
  }

  def normalGroup: P[List[Token]] = "(" ~> tokens <~ ")"

  def keywordsParser(keywords: Iterable[String]): P[(String, Range)] =
    // Sort to ensure bigger strings matched first
    keywords.toSeq.sortBy(-_.length).map(withRange(_)).reduce(_ | _)

  def elementKeyword: P[Token] =
    keywordsParser(Elements.elements.values.flatMap(_.keywords)) ^^ {
      case (word, range) => Token(Command, word, range)
    }

  def modifierKeyword: P[Token] =
    keywordsParser(Modifiers.modifiers.values.flatMap(_.keywords)) ^^ {
      case (keyword, range) =>
        val mod =
          Modifiers.modifiers.values.find(_.keywords.contains(keyword)).get
        val tokenType = mod.arity match
          case 1 => MonadicModifier
          case 2 => DyadicModifier
          case 3 => TriadicModifier
          case 4 => TetradicModifier
          case _ => SpecialModifier
        Token(tokenType, keyword, range)
    }

  def structOpener: P[Token] =
    keywordsParser(structOpeners.keys) ^^ { case (word, range) =>
      val sbcs = structOpeners(word).open
      Token(StructureOpen, sbcs, range)
    }

  def otherKeyword: P[Token] =
    keywordsParser(keywords.keys) ^^ { case (word, range) =>
      Token(keywords(word), word, range)
    }

  def litGetVariable: P[Token] =
    withRange("""\$([_a-zA-Z][_a-zA-Z0-9]*)?""".r) ^^ { case (value, range) =>
      Token(GetVar, value.substring(1), range)
    }

  def litSetVariable: P[Token] =
    withRange(""":=([_a-zA-Z][_a-zA-Z0-9]*)?""".r) ^^ { case (value, range) =>
      Token(SetVar, value.substring(2), range)
    }

  def litSetConstant: P[Token] =
    withRange(""":!=([_a-zA-Z][_a-zA-Z0-9]*)?""".r) ^^ { case (value, range) =>
      Token(Constant, value.substring(3), range)
    }

  def litAugVariable: P[Token] =
    withRange(""":>([a-zA-Z][_a-zA-Z0-9]*)?""".r) ^^ { case (value, range) =>
      Token(AugmentVar, value.substring(2), range)
    }

  def unpackVar: P[List[Token]] =
    withRange(":=") ~ list ^^ { case (_, unpackRange) ~ listTokens =>
      (Token(SyntaxTrigraph, "#:[", unpackRange) :: listTokens.slice(
        1,
        listTokens.size - 1
      )) :+ Token(UnpackClose, "]", listTokens.last.range)
    }

  def list: P[List[Token]] =
    parseToken(ListOpen, "[") ~! rep(
      not(raw"[|,\]]".r) ~> singleToken ~
        (branch | litBranch).?
    ) ~ parseToken(ListClose, "]") ^^ { case startTok ~ elems ~ endTok =>
      val middle = elems.flatMap { case elem ~ branch => elem ++ branch }
      (startTok +: middle) :+ endTok
    }

  // TODO figure out what this is for
  // def tilde: P[Token] = "~" ^^^ AlreadyCode("!")

  def litBranch: P[Token] = keywordsParser(branchKeywords) ^^ {
    case (value, range) => Token(Branch, value, range)
  }

  def litStructClose: P[Token] = keywordsParser(endKeywords) ^^ {
    case (value, range) => Token(StructureClose, value, range)
  }

  def rawCode: P[List[Token]] = withStartPos("#([^#]|#[^}])*#}".r) ^^ {
    case (value, row, col) =>
      super
        .parseAll(super.tokens, value.substring(1, value.length - 2))
        .map { tokens =>
          tokens.map { tok =>
            tok.copy(range =
              tok.range.copy(
                startRow = row + tok.range.startRow,
                startCol =
                  if tok.range.startRow == 0 then col + tok.range.startCol
                  else tok.range.startCol
              )
            )
          }
        }
        .get
  }

  def singleToken: P[List[Token]] =
    lambdaBlock | list | unpackVar | (litGetVariable | litSetVariable | litSetConstant | litAugVariable
      | elementKeyword | modifierKeyword | structOpener | otherKeyword | litBranch | litStructClose)
      .map(List(_))
      | ws | normalGroup | rawCode | super.token.map(List(_))

  override def tokens: P[List[Token]] = rep(singleToken).map(_.flatten)

end LiterateLexer
